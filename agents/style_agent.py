# [MODEL: 8B]  Provjerava PEP8, naming conventions, docstrings
"""Style and formatting review agent for code quality analysis.

This module implements a fast, lightweight agent focused on code style,
PEP8 compliance, naming conventions, and formatting improvements.
Uses Llama 3.1 8B for quick execution.
"""

import os
import json
import logging
from dotenv import load_dotenv
from langchain_groq import ChatGroq
from core.state import AgentState
from utils.common import STYLE_PROMPT

load_dotenv()

logger = logging.getLogger(__name__)


def style_node(state: AgentState) -> dict:
    """Analyzes code for style, formatting, and PEP8 compliance issues.

    This agent processes the PR diff and identifies style violations including:
    - PEP8 compliance (line length, indentation, spacing)
    - Naming conventions (snake_case vs camelCase, constants)
    - Code formatting and consistency
    - Documentation and docstring quality
    - Readability improvements

    Args:
        state: The shared AgentState containing pr_diff to analyze.

    Returns:
        A dict with 'style_comments' key containing a list of formatted
        style issue strings, ready to append to the shared state via operator.add.

    Example:
        >>> state = {"pr_diff": "code snippet...", "logic_comments": [], ...}
        >>> result = style_node(state)
        >>> result["style_comments"]
        ["**Generated by Multi-Agent PR Review System - Style Agent**\n...", "..."]
    """
    try:
        pr_diff = state.get("pr_diff", "")
        if not pr_diff:
            logger.warning("style_node: No PR diff provided")
            return {"style_comments": []}

        # Initialize the fast model (8B)
        model_name = os.getenv("MODEL_FAST", "llama-3.1-8b-instant")
        llm = ChatGroq(temperature=0, model_name=model_name)

        # Prepare the analysis prompt
        user_prompt = f"""Analyze the following code for style and formatting issues:

{pr_diff}

Apply the style review guidelines and provide findings in JSON format."""

        # Invoke the model with the system prompt and user input
        messages = [
            {"role": "system", "content": STYLE_PROMPT},
            {"role": "user", "content": user_prompt},
        ]

        response = llm.invoke(messages)
        response_text = response.content

        # Format the response as a style comment
        style_comment = f"**Generated by Multi-Agent PR Review System - Style Agent**\n\n{response_text}"

        logger.info("style_node: Analysis completed successfully")
        return {"style_comments": [style_comment]}

    except Exception as e:
        logger.error(f"style_node: Error during analysis - {e}")
        error_comment = f"**Style Agent Error**: {str(e)}"
        return {"style_comments": [error_comment]}
